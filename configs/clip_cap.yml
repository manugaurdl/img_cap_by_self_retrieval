#dynamic variables
logging : False
init_val : True
jatayu : True
lazy_load : False 
batch_size : 32
mle_bsz : 32
train_method : "scst" # ['mle', 'scst']
old_cider : False

#llama
llama_cap : True
suffix : "llama_1k_removed" #["llama", "llama_1k_removed"]

# Hyperparams
lr : 2e-5
optimizer: "AdamW"
num_epochs: 20
prefix_length : 10
prefix_dim : 512
num_layers : 8
attn_heads : 8
warmup_steps : 1000

#scst training
cider_reward_weight : 1
bleu_reward_weight : 0
cached_tokens : 'coco-train-idxs_gpt'
scst_lr : 1e-6
train_sample_n : 1
use_scheduler : False

#training 
log_train_metrics : False

#Boolean variables
save_last_epoch : True
save_best_metric : True
save_every_epoch : False
save_every_n_iter : False
freeze_gpt : False
save_val_preds : False

# wandb related variables
wandb : 
  sweep : False
  sweep_id: ""
  sweep_run_count : 100
  entity : "manugaur"
  project : "img_cap_self_retrieval"
  run_name : "clip_scst_transformer_llama_train_lr_1e-6"

#Sampling 
method : "clip_cap"
sampling_method : "greedy"
eval_sample_n : 1
#HF
hf_sampling : True
increase_gpt_vocab_by : 5000
do_sample : False
max_length : 40

# Path variables
train_data : 'data/ViT-B_32_train_emb.pkl'
val_data : 'data/ViT-B_32_val_emb.pkl'
test_data : 'data/ViT-B_32_test_emb.pkl'
out_dir : 'checkpoints/clip_ViT-B_32'
# in case of Test or scst training
load_model_from : 'checkpoints/clip_ViT-B_32'
cocotalk : 'data/cocotalk.json'
#static variables
gpu_id: 0
tokenizer : "gpt2"
normalize_prefix : False 
use_beam_search: False
entry_count : 1
top_p : 0.8
temp : 0.1
stop_token : '.'
lang_eval : 1
# Specific experiments (default = False)
reproduce_clipcap : False
train_only_ln : False


#-1.05  1
#0.145 .1
#0.011 .01
#0  .001